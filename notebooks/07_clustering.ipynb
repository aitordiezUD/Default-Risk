{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Clustering Models",
   "id": "b3496eaa1e1b5147"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First of all, we are going to import the necessary libraries.",
   "id": "5f7ed3c07ea99ceb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T09:46:21.125518Z",
     "start_time": "2024-12-12T09:46:15.552246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from cuml import NearestNeighbors\n",
    "import numpy as np\n",
    "import optuna\n",
    "import cuml\n",
    "from cuml.cluster import DBSCAN as cuDBSCAN\n",
    "from dask.array import asarray\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from cuml.metrics.cluster.silhouette_score import cython_silhouette_score as cu_silhouette_score\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.decomposition import PCA\n",
    "import joblib\n",
    "import optuna.visualization as vis\n",
    "import cupy as cp\n",
    "import numpy as np\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score, mutual_info_score, homogeneity_score, completeness_score, v_measure_score\n",
    "import os\n",
    "import cupy as cp\n",
    "import gc\n"
   ],
   "id": "12b0315dd418cf7b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aitor/anaconda3/envs/rapids-24.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First of all, we are going to create a class to compute all the metrics. This class will be used to evaluate the performance of the models using the K Fold method.\n",
   "id": "b800e451d0a3f186"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-12T09:46:21.150092Z",
     "start_time": "2024-12-12T09:46:21.142667Z"
    }
   },
   "source": [
    "\n",
    "class ClusteringMetrics:\n",
    "    def __init__(self, X, model, y_true=None):\n",
    "        if hasattr(X, 'get'):\n",
    "            self.X = pd.DataFrame(X.get()).reset_index(drop=True)\n",
    "        else:\n",
    "            self.X = pd.DataFrame(X).reset_index(drop=True)\n",
    "        self.model = model\n",
    "        self.y_true = y_true  # Guardar etiquetas verdaderas para evaluación externa (si están disponibles)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.fit(self.X)\n",
    "\n",
    "    def compute_internalEvaluation(self):\n",
    "        if hasattr(self.model, 'predict'):\n",
    "            y_pred = self.model.predict(self.X)\n",
    "        elif hasattr(self.model, 'labels_'):\n",
    "            y_pred = self.model.labels_\n",
    "        else:\n",
    "            raise AttributeError(\"The model does not have a valid method to predict cluster labels.\")\n",
    "\n",
    "        self.shiloette_score = cu_silhouette_score(cp.asarray(self.X), cp.asarray(y_pred))\n",
    "        self.calinski_harabasz_score = calinski_harabasz_score(self.X, y_pred)\n",
    "        self.davies_bouldin_score = davies_bouldin_score(self.X, y_pred)\n",
    "\n",
    "    def compute_externalEvaluation(self):\n",
    "        if self.y_true is None:\n",
    "            raise ValueError(\"Ground truth labels (y_true) are required for external evaluation.\")\n",
    "\n",
    "        if hasattr(self.model, 'predict'):\n",
    "            y_pred = self.model.predict(self.X)\n",
    "        elif hasattr(self.model, 'labels_'):\n",
    "            y_pred = self.model.labels_\n",
    "        else:\n",
    "            raise AttributeError(\"The model does not have a valid method to predict cluster labels.\")\n",
    "\n",
    "        self.adjusted_rand = adjusted_rand_score(self.y_true, y_pred)\n",
    "        self.mutual_info = mutual_info_score(self.y_true, y_pred)\n",
    "        self.homogeneity = homogeneity_score(self.y_true, y_pred)\n",
    "        self.completeness = completeness_score(self.y_true, y_pred)\n",
    "        self.v_measure = v_measure_score(self.y_true, y_pred)\n",
    "\n",
    "    def print_metrics(self):\n",
    "        print(f\"Silhouette Score: {self.shiloette_score}\")\n",
    "        print(f\"Calinski Harabasz Score: {self.calinski_harabasz_score}\")\n",
    "        print(f\"Davies Bouldin Score: {self.davies_bouldin_score}\")\n",
    "        if hasattr(self, 'adjusted_rand'):\n",
    "            print(f\"Adjusted Rand Index: {self.adjusted_rand}\")\n",
    "            print(f\"Mutual Information Score: {self.mutual_info}\")\n",
    "            print(f\"Homogeneity Score: {self.homogeneity}\")\n",
    "            print(f\"Completeness Score: {self.completeness}\")\n",
    "            print(f\"V-Measure Score: {self.v_measure}\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        directory = os.path.dirname(path)\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        joblib.dump(self.model, path)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loading the data",
   "id": "de7502fa94451e22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T09:46:35.229003Z",
     "start_time": "2024-12-12T09:46:35.022026Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_parquet('../data/processed/selected_features_df.parquet')",
   "id": "576a7235adaad1f1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are going to scale the data using the StandardScaler.",
   "id": "60ac32689baa2654"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T09:46:37.349689Z",
     "start_time": "2024-12-12T09:46:36.316559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)"
   ],
   "id": "928738ec36b678c3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's calculate the Hopkins statistic to check if the data is suitable for clustering.",
   "id": "b5d673e3bcff2abf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T12:08:14.170645Z",
     "start_time": "2024-12-07T12:08:14.166537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hopkins(X):\n",
    "    d = X.shape[1]\n",
    "    n = len(X)\n",
    "    m = int(0.1 * n)\n",
    "    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n",
    "\n",
    "    rand_X = np.random.rand(n, d)\n",
    "    ujd = []\n",
    "    wjd = []\n",
    "\n",
    "    for j in range(m):\n",
    "        u_dist, _ = nbrs.kneighbors(rand_X[j].reshape(1, -1), 2, return_distance=True)\n",
    "        ujd.append(u_dist[0][1])\n",
    "        w_dist, _ = nbrs.kneighbors(X.iloc[j].values.reshape(1, -1), 2, return_distance=True)\n",
    "        wjd.append(w_dist[0][1])\n",
    "\n",
    "    H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
    "    if np.isnan(H):\n",
    "        print(ujd, wjd)\n",
    "        H = 0\n",
    "\n",
    "    return H"
   ],
   "id": "610c16f1920a6551",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Hopkins statistic ranges from 0 to 1. The closer to 1, the more suitable the data is for clustering. Let's calculate the Hopkins statistic for the data.",
   "id": "70f6a58c1dce40d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T12:28:57.836930Z",
     "start_time": "2024-12-06T12:24:11.351507Z"
    }
   },
   "cell_type": "code",
   "source": "hopkins(df)",
   "id": "c0f81e8bd8481e75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9645847043701986"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Hopkins statistic is close to 1, so the data is suitable for clustering.",
   "id": "83ed865ab2c3563b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DBSCAN",
   "id": "b5e21add7827ba3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are going to use the DBSCAN algorithm to cluster the data. We are going to use the Optuna library to find the best hyperparameters.",
   "id": "5d67868114006bca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T14:36:01.513839Z",
     "start_time": "2024-12-06T13:29:53.349126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def objective(trial):\n",
    "    eps = trial.suggest_float('eps', 0.1, 1.0)\n",
    "    min_samples = trial.suggest_int('min_samples', 5, 10)\n",
    "    metric = trial.suggest_categorical('metric', ['euclidean', 'cosine'])\n",
    "\n",
    "    model = cuDBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
    "    model.fit(df_scaled)\n",
    "    y_pred = model.labels_\n",
    "\n",
    "    # Verificar si hay al menos 2 clusters\n",
    "    if len(cp.unique(y_pred)) > 1:  # Usamos cupy para manejar el arreglo en GPU\n",
    "        return silhouette_score(df_scaled, y_pred)  # Devuelve el Silhouette score si hay más de 1 cluster\n",
    "    else:\n",
    "        return -1  # Devuelve un valor negativo si solo se encuentra un único cluster\n",
    "    study = optuna.create_study(direction='maximize', study_name='dbscan')\n",
    "    study.optimize(objective, n_trials=5)\n",
    "    print(f\"Best parameters: {study.best_params}\")"
   ],
   "id": "716915c295157c48",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-06 14:29:53,352] A new study created in memory with name: dbscan\n",
      "[I 2024-12-06 14:46:08,489] Trial 0 finished with value: 0.06344437831203903 and parameters: {'eps': 0.45560417786879226, 'min_samples': 9, 'metric': 'cosine'}. Best is trial 0 with value: 0.06344437831203903.\n",
      "[I 2024-12-06 15:01:01,110] Trial 1 finished with value: -0.08649164881054881 and parameters: {'eps': 0.2812818251401415, 'min_samples': 9, 'metric': 'cosine'}. Best is trial 0 with value: 0.06344437831203903.\n",
      "[I 2024-12-06 15:05:37,931] Trial 2 finished with value: -1.0 and parameters: {'eps': 0.15472718863535367, 'min_samples': 10, 'metric': 'euclidean'}. Best is trial 0 with value: 0.06344437831203903.\n",
      "[I 2024-12-06 15:20:50,951] Trial 3 finished with value: 0.12279910760197582 and parameters: {'eps': 0.529945149343017, 'min_samples': 10, 'metric': 'cosine'}. Best is trial 3 with value: 0.12279910760197582.\n",
      "[I 2024-12-06 15:36:01,502] Trial 4 finished with value: 0.15272329032663132 and parameters: {'eps': 0.9095320672300284, 'min_samples': 8, 'metric': 'cosine'}. Best is trial 4 with value: 0.15272329032663132.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'eps': 0.9095320672300284, 'min_samples': 8, 'metric': 'cosine'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can see that the bigger the eps, the better the silhouette score. Also, the cosine metric is better than the euclidean metric for this problem. Taking this into account we are going to create another Optuna study to find the best hyperparameters.",
   "id": "24332028cd3161fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-07T09:52:46.671717Z",
     "start_time": "2024-12-07T09:09:25.233474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_scaled = cp.asarray(df_scaled)\n",
    "\n",
    "def objective(trial):\n",
    "    gc.collect()\n",
    "    cp._default_memory_pool.free_all_blocks()\n",
    "\n",
    "    eps = trial.suggest_float('eps', 1, 1.5)\n",
    "    min_samples = trial.suggest_int('min_samples', 5, 10)\n",
    "\n",
    "    model = cuDBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "    model.fit(df_scaled)\n",
    "    y_pred = model.labels_\n",
    "\n",
    "    # Verificar si hay al menos 2 clusters\n",
    "    if len(cp.unique(y_pred)) > 1:  # Usamos cupy para manejar el arreglo en GPU\n",
    "        return cu_silhouette_score(df_scaled, y_pred)  # Devuelve el Silhouette score si hay más de 1 cluster\n",
    "    else:\n",
    "        return -1  # Devuelve un valor negativo si solo se encuentra un único cluster\n",
    "study = optuna.create_study(direction='maximize', study_name='dbscan')\n",
    "study.optimize(objective, n_trials=5)\n",
    "print(f\"Best parameters: {study.best_params}\")"
   ],
   "id": "e79f9ad4d94a5228",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-07 10:09:25,235] A new study created in memory with name: dbscan\n",
      "[I 2024-12-07 10:18:00,067] Trial 0 finished with value: 0.35992020990680346 and parameters: {'eps': 1.468031449624661, 'min_samples': 9}. Best is trial 0 with value: 0.35992020990680346.\n",
      "[I 2024-12-07 10:26:35,946] Trial 1 finished with value: 0.37549860352774733 and parameters: {'eps': 1.2027419903680259, 'min_samples': 8}. Best is trial 1 with value: 0.37549860352774733.\n",
      "[I 2024-12-07 10:35:13,500] Trial 2 finished with value: 0.38383985601186843 and parameters: {'eps': 1.2125524450750875, 'min_samples': 7}. Best is trial 2 with value: 0.38383985601186843.\n",
      "[I 2024-12-07 10:43:59,112] Trial 3 finished with value: 0.35992020990680346 and parameters: {'eps': 1.4344474832899103, 'min_samples': 6}. Best is trial 2 with value: 0.38383985601186843.\n",
      "[I 2024-12-07 10:52:46,669] Trial 4 finished with value: 0.1540651024846537 and parameters: {'eps': 1.001327955535295, 'min_samples': 6}. Best is trial 2 with value: 0.38383985601186843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'eps': 1.2125524450750875, 'min_samples': 7}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T22:24:08.772042Z",
     "start_time": "2024-12-10T22:15:19.873809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = cuDBSCAN(**study.best_params)\n",
    "metrics = ClusteringMetrics(df_scaled, model)\n",
    "metrics.train()\n",
    "metrics.save_model('../models/dbscan.pkl')\n",
    "metrics.compute_internalEvaluation()\n",
    "metrics.print_metrics()"
   ],
   "id": "d721b417034b6187",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.38383985601186843\n",
      "Calinski Harabasz Score: 6.71393677535788\n",
      "Davies Bouldin Score: 1.5755187609143613\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot the study results",
   "id": "c84ff0b910eb0ab8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_optimization_history(study)",
   "id": "ed36c42cfea58d78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_parallel_coordinate(study)",
   "id": "892c03270c4480d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_slice(study)",
   "id": "6a4cbe660b3cb66a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_param_importances(study)",
   "id": "2e06c16f0b85f60c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_contour(study)",
   "id": "e8d290b60e95c7a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# BIRCH",
   "id": "69a53b5fbe03692c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we are having MemoryError using this algorithm, we are going to take a sample of the original dataset in order to execute this algorithm. The problem we are having is the following:\n",
    "\n",
    "MemoryError: Unable to allocate 37.2 GiB for an array with shape (4996850496,) and data type float64\n",
    "\n",
    "The sample size we are using is 45000, which enables to run the algorithm with no errors and in a reasonable training time."
   ],
   "id": "42e281f4f395046e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_scaled_reduced = df_scaled.sample(n=45000, random_state=42)",
   "id": "7cd6642546d59e5a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are going to use Birch algorithm. For the hyperparameter tuning we are going to perform a study using Optuna framework.",
   "id": "91b84800c7514afc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import optuna\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def objective(trial):\n",
    "    n_clusters = trial.suggest_int(\"n_clusters\", 2, 10)\n",
    "    threshold = trial.suggest_float(\"threshold\", 0.1, 2.0, step = 0.1)\n",
    "    branching_factor = trial.suggest_int(\"branching_factor\", 10, 50)\n",
    "\n",
    "    birch = Birch(n_clusters=n_clusters, threshold=threshold, branching_factor=branching_factor)\n",
    "    labels = birch.fit_predict(df_scaled_reduced)\n",
    "    score = silhouette_score(df_scaled_reduced, labels)\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"birch\")\n",
    "study.optimize(objective, n_trials=5)"
   ],
   "id": "c0f2f404fb6a18e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Computing the metrics for the best hyperparameters",
   "id": "798a59cdc25a9023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = Birch(**study.best_params)\n",
    "metrics = ClusteringMetrics(df_scaled, model)\n",
    "metrics.train()\n",
    "metrics.save_model('../models/dbscan.pkl')\n",
    "metrics.compute_internalEvaluation()\n",
    "metrics.print_metrics()"
   ],
   "id": "f3e7afd56951a0a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot the study results",
   "id": "8ac2371b14f3098d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_optimization_history(study)\n",
   "id": "229fff30976e3cd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_parallel_coordinate(study)",
   "id": "7fbf2e3a4892df9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_slice(study)",
   "id": "a42629b0a2942569"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_param_importances(study)",
   "id": "d4c55eb6c692a5c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_contour(study)",
   "id": "c34b5c53395fee10"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Gaussian Mixture",
   "id": "dba0b245cb4fbcff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are going to use the Gaussian Mixture algorithm to cluster the data. We are going to use the Optuna library to find the best hyperparameters.",
   "id": "86e1a197036c069c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:02:21.816098Z",
     "start_time": "2024-12-10T20:16:58.385382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def objective(trial):\n",
    "    n_components = trial.suggest_int('n_components', 2, 5)\n",
    "    covariance_type = trial.suggest_categorical('covariance_type', ['full', 'tied', 'diag', 'spherical'])\n",
    "    tol = trial.suggest_float('tol', 1e-5, 1e-1)\n",
    "    reg_covar = trial.suggest_float('reg_covar', 1e-6, 1e-2)\n",
    "\n",
    "\n",
    "    model = GaussianMixture(n_components=n_components, covariance_type=covariance_type, tol=tol, reg_covar=reg_covar, random_state=42)\n",
    "    model.fit(df_scaled)\n",
    "    y_pred = model.predict(df_scaled)\n",
    "\n",
    "    return silhouette_score(df_scaled, y_pred)\n",
    "\n",
    "study = optuna.create_study(direction='maximize', study_name='gmm')\n",
    "study.optimize(objective, n_trials=5)"
   ],
   "id": "178ccff80759a332",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-10 21:16:58,431] A new study created in memory with name: gmm\n",
      "[I 2024-12-10 21:26:26,832] Trial 0 finished with value: 0.068944020380107 and parameters: {'n_components': 2, 'covariance_type': 'tied', 'tol': 0.02792127136333412, 'reg_covar': 0.007863629516834955}. Best is trial 0 with value: 0.068944020380107.\n",
      "[I 2024-12-10 21:35:01,073] Trial 1 finished with value: 0.06353784088589444 and parameters: {'n_components': 5, 'covariance_type': 'diag', 'tol': 0.02418755280161164, 'reg_covar': 0.0006082345442615693}. Best is trial 0 with value: 0.068944020380107.\n",
      "[I 2024-12-10 21:43:37,780] Trial 2 finished with value: 0.03497509670255373 and parameters: {'n_components': 3, 'covariance_type': 'spherical', 'tol': 0.0444942314395488, 'reg_covar': 0.009547804964557524}. Best is trial 0 with value: 0.068944020380107.\n",
      "[I 2024-12-10 21:53:19,753] Trial 3 finished with value: 0.06340238645125286 and parameters: {'n_components': 2, 'covariance_type': 'spherical', 'tol': 0.027655080965647047, 'reg_covar': 0.001014408894137987}. Best is trial 0 with value: 0.068944020380107.\n",
      "[I 2024-12-10 22:02:21,813] Trial 4 finished with value: 0.05952454746783261 and parameters: {'n_components': 3, 'covariance_type': 'full', 'tol': 0.0016969742230608445, 'reg_covar': 0.004195819448381651}. Best is trial 0 with value: 0.068944020380107.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Computing the metrics for the best parameters.",
   "id": "212d8c1856a3aec4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:39:26.396620Z",
     "start_time": "2024-12-10T21:35:43.149604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = GaussianMixture(**study.best_params, random_state=42)\n",
    "\n",
    "metrics = ClusteringMetrics(df_scaled, model)\n",
    "metrics.train()\n",
    "metrics.compute_internalEvaluation()\n",
    "metrics.save_model('../models/gmm.pkl')\n",
    "metrics.print_metrics()"
   ],
   "id": "b0517a3ba050f2bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.06894402038010695\n",
      "Calinski Harabasz Score: 19626.39283621834\n",
      "Davies Bouldin Score: 2.5051416177208856\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T21:58:33.423775Z",
     "start_time": "2024-12-10T21:53:02.197303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model = joblib.load('../models/gmm.pkl')\n",
    "#\n",
    "# metrics = ClusteringMetrics(df_scaled, model)\n",
    "# metrics.compute_internalEvaluation()\n",
    "# metrics.print_metrics()"
   ],
   "id": "edb3e3e52138fd6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.06894402038010695\n",
      "Calinski Harabasz Score: 19626.39283621834\n",
      "Davies Bouldin Score: 2.5051416177208856\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Plot the study results.",
   "id": "669da7b0cfafc30e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_optimization_history(study)\n",
   "id": "75ecdc23aa464553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_parallel_coordinate(study)",
   "id": "df455e59bcc7b4da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_slice(study)",
   "id": "67a851e92b73a5b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_param_importances(study)",
   "id": "bf556107458aa192"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "vis.plot_contour(study)",
   "id": "f0f30934d9f4cc89"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
